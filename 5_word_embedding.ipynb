{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2969a31d-55f9-473b-92af-ed8b88dd9059",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4335677d-0247-46d9-b93f-298f9ee4eafa",
   "metadata": {},
   "source": [
    "We learned a few ways to extract features from text data, and represent them as vectors. For the purposes of large language models and more ambitious natural language processing projects, word embeddings are a critical technique that will serve as a capstone to this course. \n",
    "\n",
    "**Word embeddings** are a technique for representing similar or related words to be close together in a vector space. It has allowed many breakthroughs in natural language processing, including the large language models we know today. The technical advantage they bring is they clump together words that are related and therefore reduce the number of dimensions needed, and by creating this density there is more data available for a given context. This greatly reduces the dimensionality and the number of features. \n",
    "\n",
    "Each word is mapped to a vector, and that vector takes up a point in space. Words that tend to be used together in a similar context are going to have vectors that are closer together. For example, \"dog\" and \"dalmation\" are likely to be close together. However, \"dog\" and \"cat\" are also going to be close together because those two words are often used together in the same sentence/context as well. The word \"pet\" should be close to those vectors as well. \n",
    "\n",
    "\"Candy\" and \"Chocolate\" will be close together, but not be anywhere near the \"dog\", \"pet\", or \"cat\"-related words. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0cc954-d473-4137-8f0b-e52420b4f727",
   "metadata": {},
   "source": [
    "![svg image](data:image/svg+xml;base64,<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!-- Created with Inkscape (http://www.inkscape.org/) -->

<svg
   width="126.52377mm"
   height="126.52377mm"
   viewBox="0 0 126.52377 126.52377"
   version="1.1"
   id="svg1"
   inkscape:version="1.3.1 (91b66b0, 2023-11-16)"
   sodipodi:docname="89maFwKN.svg"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:svg="http://www.w3.org/2000/svg">
  <sodipodi:namedview
     id="namedview1"
     pagecolor="#ffffff"
     bordercolor="#000000"
     borderopacity="0.25"
     inkscape:showpageshadow="2"
     inkscape:pageopacity="0.0"
     inkscape:pagecheckerboard="0"
     inkscape:deskcolor="#d1d1d1"
     inkscape:document-units="mm"
     inkscape:zoom="1.0387876"
     inkscape:cx="279.65293"
     inkscape:cy="233.44522"
     inkscape:window-width="1392"
     inkscape:window-height="1212"
     inkscape:window-x="2239"
     inkscape:window-y="25"
     inkscape:window-maximized="0"
     inkscape:current-layer="layer1" />
  <defs
     id="defs1" />
  <g
     inkscape:label="Layer 1"
     inkscape:groupmode="layer"
     id="layer1"
     transform="translate(-15.951054,-17.224575)">
    <rect
       style="fill:none;stroke:#000000;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
       id="rect1"
       width="125.82377"
       height="125.82377"
       x="16.301054"
       y="17.574575" />
    <path
       style="fill:#ff0000;stroke:#ff0000;stroke-width:0.834212;stroke-linecap:round;stroke-linejoin:round"
       d="M 16.36816,143.33123 47.562539,33.093973"
       id="path1" />
    <path
       style="fill:#ff8080;stroke:#ff8080;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
       d="M 16.301054,143.39834 51.450205,43.299678"
       id="path2" />
    <path
       style="fill:#ffd5d5;stroke:#ffaaaa;stroke-width:0.794599;stroke-linecap:round;stroke-linejoin:round"
       d="M 16.348353,143.35104 54.829303,58.119808"
       id="path3" />
    <path
       style="fill:#800000;stroke:#aa0000;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
       d="M 16.301054,143.39834 33.87563,39.479116"
       id="path4" />
    <path
       style="fill:#0000ff;stroke:#0000ff;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
       d="M 16.301054,143.39834 108.7586,101.88159"
       id="path5" />
    <text
       xml:space="preserve"
       style="font-size:4.9389px;font-family:Oswald;-inkscape-font-specification:Oswald;fill:#0000ff;stroke:#0000ff;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
       x="30.602945"
       y="36.677376"
       id="text5"><tspan
         sodipodi:role="line"
         id="tspan5"
         style="fill:#800000;stroke:none;stroke-width:0.7"
         x="30.602945"
         y="36.677376">Dog</tspan></text>
    <text
       xml:space="preserve"
       style="font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:4.9389px;font-family:Oswald;-inkscape-font-specification:'Oswald, Normal';font-variant-ligatures:normal;font-variant-caps:normal;font-variant-numeric:normal;font-variant-east-asian:normal;fill:#ff0000;stroke:none;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
       x="44.993721"
       y="29.475426"
       id="text5-9"><tspan
         sodipodi:role="line"
         id="tspan1">Dalmation</tspan></text>
    <text
       xml:space="preserve"
       style="font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:4.9389px;font-family:Oswald;-inkscape-font-specification:'Oswald, Normal';font-variant-ligatures:normal;font-variant-caps:normal;font-variant-numeric:normal;font-variant-east-asian:normal;fill:#ff5555;stroke:none;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
       x="52.19659"
       y="41.109615"
       id="text5-9-3"><tspan
         sodipodi:role="line"
         id="tspan7"
         x="52.19659"
         y="41.109615">Pet</tspan></text>
    <text
       xml:space="preserve"
       style="font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:4.9389px;font-family:Oswald;-inkscape-font-specification:'Oswald, Normal';font-variant-ligatures:normal;font-variant-caps:normal;font-variant-numeric:normal;font-variant-east-asian:normal;fill:#ff8080;stroke:none;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
       x="56.780956"
       y="55.135323"
       id="text5-9-3-8"><tspan
         sodipodi:role="line"
         id="tspan8"
         x="56.780956"
         y="55.135323">Cat</tspan></text>
    <text
       xml:space="preserve"
       style="font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:4.9389px;font-family:Oswald;-inkscape-font-specification:'Oswald, Normal';font-variant-ligatures:normal;font-variant-caps:normal;font-variant-numeric:normal;font-variant-east-asian:normal;fill:#0000ff;stroke:none;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
       x="114.30407"
       y="101.49208"
       id="text5-9-3-4"><tspan
         sodipodi:role="line"
         id="tspan9"
         x="114.30407"
         y="101.49208">Candy</tspan></text>
    <text
       xml:space="preserve"
       style="font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:4.9389px;font-family:Oswald;-inkscape-font-specification:'Oswald, Normal';font-variant-ligatures:normal;font-variant-caps:normal;font-variant-numeric:normal;font-variant-east-asian:normal;fill:#0066ff;stroke:none;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
       x="109.02439"
       y="115.92626"
       id="text5-9-3-4-8"><tspan
         sodipodi:role="line"
         id="tspan10"
         x="109.02439"
         y="115.92626">Chocolate</tspan></text>
    <path
       style="fill:#5599ff;stroke:#2a7fff;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
       d="M 16.36816,143.33123 105,114.61679"
       id="path9" />
  </g>
</svg>
)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75294571-7f85-434a-b3d3-ee82b35b1e19",
   "metadata": {},
   "source": [
    "So how are these word embeddings built? Generally speaking, a neural network or other algorithm will look around a word at the other words surrounding it. Then you will have a model that for a given word input, will output words with probabilities that they are associated with that word, or perhaps are the next word in a sentence. Think of a given input word outputting a probability distribution of other words that would surround it. Given a large enough text dataset, enough context can be constructed to productively predict relevant words. \n",
    "\n",
    "What gets really interesting is how contexts can be navigated. If you have a sufficient word embedding model you can take the vector for \"king,\" subtract the vector for \"man,\" add the vector for \"woman,\" and then land close to the vector for \"queen.\" Similarly, if I take the vector for \"shirt,\" subtract the vector \"man,\" then add the vector \"woman\" I might get the word \"blouse.\" Or if I take the vector for \"Berlin\" and subtract \"Germany,\" then add \"England\" I *should* get \"London.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532526ac-c705-4be2-b977-46df6472d9b8",
   "metadata": {},
   "source": [
    "## Word2Vec and GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdaf0df-f63d-4f66-9262-520d65bad957",
   "metadata": {},
   "source": [
    "Back in 2013, Tomas Mikolov at Google developed a famous method [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) to build word embeddings. It has become the standard for word embeddings and many extensions like GloVe have been built around it. It can build two types of models for word embeddings: \n",
    "\n",
    "* CBOW - Continuous bag of words, builds a word embedding by predicting the current word based on its context.\n",
    "* Continuous Skip-Gram Model - Builds a word embedding by predicting the surrounding words given a current word.\n",
    "\n",
    "Word2Vec takes each current word and looks at a window of neighboring words, and the size of this window is configurable. Because of the efficiency of the algorithm, larger embeddings can be built efficiently from larger amounts of training data. A lot of the previous techniques we learned require sparse vectors with lots of 0's, and this is not ideal as we want dense datasets with more information and less 0's. \n",
    "\n",
    "An extension to Word2Vec, Global Vectors for Word Representation ([GloVe](https://en.wikipedia.org/wiki/GloVe)) was further developed in 2014 by researchers at Stanford. It incorporates Latent Semantic Analysis (LSA) and other techniques that better incorporate global word statistics. Instead of using a window for local context, it uses statistics across the entire text corpus. This results in substantial improvements to word embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fde635-29fb-4e02-bd88-c99eb2c12b1c",
   "metadata": {},
   "source": [
    "## Word Embedding with Gensim "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc0a640-6384-4662-97f0-2b5815b17d85",
   "metadata": {},
   "source": [
    "To build a useful word embedding, you will need a large amount of text data. Think millions or billions of words. You can create a word embedding that is reusable for multiple models, or marry one to a specific application model where it learns jointly with it. We will take the former approach, and as you can guess pre-built word embeddings are available for free from researchers under permissive licenses. In practice, this might be desirable instead of building your own from scratch. You can also update the embedding during the training of your own model so it is better tailored to your purposes. \n",
    "\n",
    "During the \"learning\" phase of word embedding, each word vector is moved around the vector space to be near other words relevant to it. We are going to leverage [Gensim](https://radimrehurek.com/gensim/) to manage this task for us. You can install it using `pip` or `conda` as shown below:\n",
    "\n",
    "```\n",
    "pip install gensim\n",
    "\n",
    "conda install -c conda-forge gensim\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e83b61e-6f90-4c2a-91d4-fde8ff889b16",
   "metadata": {},
   "source": [
    "### Building a Word2Vec from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62eb8d5-fff8-4eee-bdcf-6566c1c86ff3",
   "metadata": {},
   "source": [
    "To use the Word2Vec model, you need a lot of data, such as every aggregated news article or the entirety of Wikipedia. But we can still try to learn a few things still on a smaller dataset, such as the entire Charles Dickens' book *A Tale of Two Cities*. Let's download and load the text format of the book from [Project Gutenberg](https://www.gutenberg.org/ebooks/98). Let's also remove the license boilerplate, and make the words lowercase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c009546-21e3-4cbd-8c96-f1aba01e4ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    r\"https://raw.githubusercontent.com/thomasnield/machine-learning-demo-data/master/llm/tale_of_two_cities.txt\", \n",
    "    \"tale_of_two_cities.txt\"\n",
    ")\n",
    "\n",
    "\n",
    "filename = 'tale_of_two_cities.txt' \n",
    "file = open(filename, encoding=\"utf-8\")\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "text = re.sub(r\"^(.|\\n)+START OF THE PROJECT GUTENBERG EBOOK A TALE OF TWO CITIES \\*{3}\", '', text)\n",
    "text = re.sub(r\"\\*{3} END OF THE PROJECT GUTENBERG EBOOK A TALE OF TWO CITIES (.|\\n)+\", '', text)\n",
    "text = text.strip().lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731c55af-7921-4735-bad7-a64e8dd6a4f6",
   "metadata": {},
   "source": [
    "We will need to break up the book into sentences, and then the tokens within those sentences, to create a two-dimensional list. We will also filter out punctuation and stop words to keep things simple here as our dataset is sparse enough. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba00f5a-9749-494d-abc9-18774afcae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "tale_of_two_cities = nlp(text) \n",
    "\n",
    "sentences = [[token.lemma_ for token in sent if token.is_alpha and not token.is_stop] \n",
    "                           for sent in tale_of_two_cities.sents]\n",
    "\n",
    "# display each sentence and its tokens \n",
    "for sent in sentences:\n",
    "    print(f\"{sent}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8dde82-634d-4584-bc7e-1d9cd7e316ff",
   "metadata": {},
   "source": [
    "So let's get this book loaded into `Word2Vec`. There are a number of paramters we can set. \n",
    "\n",
    "* vector_size (default 100) - This sets the size of the word vector, or the number of dimensions of the word embedding.\n",
    "* window (default 5) - The maximum distance between the target word and the words around it.\n",
    "* min_count (default 5) - The minimum number of instances a word must occur to be captured for the model.\n",
    "* workers (default 3) - The number of CPU worker threads to build the word embeddings.\n",
    "* sg (default 0) - Selects the CBOW (0) or skip gram (1) algorithm for training.\n",
    "\n",
    "I did some experimentation and considering a single Charles Dickens novel is not a lot of text data for word embeddings, I found the following model to develop meaningful results. I then look up the words that are most similar to \"wine.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c633d9-f754-4a5e-a9d8-ec27d5756698",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# train model\n",
    "model = Word2Vec(sentences, vector_size=50, min_count=5, window=5, sg=1)\n",
    "\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "\n",
    "# look up words related to bastille \n",
    "model.wv.most_similar(positive=['wine'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8ccaa9-4495-47e3-aebe-3cefc8177645",
   "metadata": {},
   "source": [
    "Now you may look at these words and scratch your head. Words like \"shop\" and \"defarge\" are related to the word \"wine\"? Well if you read the book (particularly Chapter 5) many of these words make sense to be associated with the word \"wine.\" The chapter is literally titled \"The Wine-shop\" which is the setting. You can read the [Cliffs Notes](https://www.cliffsnotes.com/literature/t/a-tale-of-two-cities/summary-and-analysis/book-1-chapter-5) for a quick summary, but essentially a wine cask in **Saint Antoine** breaks in the **street** and crowds of people descend to scoop up the free wine. **Monsieur** and **Madame Defarge** are the **keepers** of the wine **shop** and talk to three men all named **Jacque**. They then unlock a **door** to a back room for their other guests. \n",
    "\n",
    "So, that explains a lot why the word embedding associated **wine** with all these words. Unfortunately, the book does not provide any further context about wine to associate words like red and white, or Cabernet and Chardonnay. We need a much larger dataset to do that. Thankfully, we can get a prebuilt word embedding that was constructed off the entire Google News dataset, which has about 100 billion words. \n",
    "\n",
    "Let's see how \"wine\" performs there. This may take a moment to download. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aeeefa-7ad5-4683-88c6-8b802c4c0603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "wv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3f157a-5af4-4de6-b447-736170ccfa9a",
   "metadata": {},
   "source": [
    "Now let's see what happens when we look up \"wine.\" Sure enough, we get some words we strongly associate with wine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b20827c-bce0-4762-82d5-554f17263c8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(wv.most_similar(positive=['wine'], topn=25))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b398c939-a063-4135-99dc-3fe0c4f65091",
   "metadata": {},
   "source": [
    "To make it extra interesting, lets navigate the vector space to find what `wine + hops - grapes` will do. Sure enough, we get `beer` as our top result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e96e03-2da0-4319-886f-55369ba78004",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wv.most_similar(positive=['wine', 'hops'], negative=['grapes'], topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7a5aeb-524b-4ce4-9953-24b07dc6aeb5",
   "metadata": {},
   "source": [
    "There are some other interesting words that were in the results. \"Stumptown Coffee?\" That's a brand of cold brew coffee that is marketed to look like a beer, but it is not beer. Still, that loosely expalins why it ended up in the word embeddings for \"beer.\" \n",
    "\n",
    "And of course, there's the classic `king + woman - man` example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99210888-935e-4564-a7da-565a540d8879",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1284d753-fc92-417e-9862-4b2464958e2c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Word embeddings are a powerful tool that are foundational for large language models and many natural langauge processing problems in general. Understanding how they are built behind-the-scenes is bit of a rabbit hole, but knowing the concepts and what they achieve is enough to make you productive. That being said, experiment and play with word embeddings, and [definitely spend some time in the Gensim documentation](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861cf59e-a7fd-40ce-a8c8-371623a029fe",
   "metadata": {},
   "source": [
    "## Exercise \n",
    "\n",
    "Calculate a vector that finds Sony's equivalent competing product for Microsoft Xbox, using the word embedding model built from Google News. Replace the question marks \"?\" below to achieve this objective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f3090a-0e40-444b-8c27-33ba8e9eaa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "\n",
    "# put your code here \n",
    "wv.most_similar(positive=[?], negative=[?], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fc090b-84a5-429d-bb97-1ca2480c05ca",
   "metadata": {},
   "source": [
    "### SCROLL DOWN FOR ANSWER\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "v "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8a72fa-7003-4c5e-b6e0-e4a047099e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "\n",
    "# put your code here \n",
    "wv.most_similar(positive=['Xbox', 'Sony'], negative=['Microsoft'], topn=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
