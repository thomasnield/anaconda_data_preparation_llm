{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c70446a-288a-4ef8-8285-ef036e2c465e",
   "metadata": {},
   "source": [
    "# Cleaning Data for LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0b00f5-31ed-4a3e-850f-03efdb1e8fdb",
   "metadata": {},
   "source": [
    "It is unreasonable to expect taking raw text from a variety of sources and expect them to be ready for large language models. There are a series of steps to get the data ready, from cleaning to vectorizing it. We will focus on cleaning the text data first, covering NLTK and spAcy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8ace1e-ed13-40ea-8a4a-4778eeb882d1",
   "metadata": {},
   "source": [
    "## The Legend of Sleepy Hollow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d449e581-12a1-4712-8322-37357bc4a948",
   "metadata": {},
   "source": [
    "For this example, we are going to download the American short story *The Legend of Sleepy Hollow* by Washington Irving. A plain text format [can be found easily at Project Gutenberg](https://www.gutenberg.org/ebooks/41) but we have it downloaded with this notebook for convenience. Let's load the file contents as a string into the `text` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6d6b12-4a99-4c46-850e-1693ef7a2e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'legend_of_sleepy_hollow.txt' \n",
    "file = open(filename, encoding=\"utf-8\")\n",
    "text = file.read()\n",
    "file.close()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b29603-08bf-4665-a6fc-2c3c3186c8f0",
   "metadata": {},
   "source": [
    "Let's then display the contents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e474766a-991e-494b-8fd0-61b0541dce93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# display the text \n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ec6e3c-e67f-4c9b-955e-4a60112447d4",
   "metadata": {},
   "source": [
    "Here we can make some observations about our data. \n",
    "\n",
    "* Thankfully this is pretty clean text and we do not have to clean up any HTML, PDF markup, or other boilerplate here.\n",
    "* There is some boilerplate for licensing and other metadata which we may want to remove.\n",
    "* This book is in English and was not translated from another language.\n",
    "* We do not anticipate spelling or grammar mistakes.\n",
    "* There are some interesting hyphenations and historical spellings like \"red-tipt\" and \"yellow-tipt.\"\n",
    "* We also have frequent uses of newline `\\n` characters and these are artificially injected at every 70 characters.\n",
    "* There do not seem to be numbers, or at least enough of them, that we have to handle.\n",
    "* There are names in this document, like Yost Van Houten.\n",
    "\n",
    "There is a lot more going on here but this is simple enough to get us started. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d405b0aa-3779-47c8-9329-6cb551a4fcaf",
   "metadata": {},
   "source": [
    "If we open up the text file directly in a text editor we will see there are license boilerplate before line 27 and after line 1159. It might be easier to use the keywords that end and start these boilerplate sections respectively. We can use some regular expression patterns for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bd16cc-ef3f-4b8f-9b79-0b7cf6afd25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "text = re.sub(r\"^(.|\\n)+START OF THE PROJECT GUTENBERG EBOOK THE LEGEND OF SLEEPY HOLLOW \\*{3}\", '', text)\n",
    "text = re.sub(r\"\\*{3} END OF THE PROJECT GUTENBERG EBOOK THE LEGEND OF SLEEPY HOLLOW (.|\\n)+\", '', text)\n",
    "text = text.strip()\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1570c4-a8a3-4b78-aa4f-10f13c2fbd54",
   "metadata": {},
   "source": [
    "For this example, we are going to download the American short story *The Legend of Sleepy Hollow* by Washington Irving. A plain text format [can be found easily at Project Gutenberg](https://www.gutenberg.org/ebooks/41) but we have it downloaded with this notebook for convenience. Let's load the file contents as a string into the `text` variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fb3d6d-f276-43bc-8d3d-ea67ebfc0488",
   "metadata": {},
   "source": [
    "## Manual Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035a6d7c-7348-4169-bd71-db1c85805ec8",
   "metadata": {},
   "source": [
    "Understandably, if we want to meaningfully prepare this data we will need to split up the words. We will learn how to do this from scratch in Python to understand the process a little bit before we bring in libraries to help us. \n",
    "\n",
    "Let's remove the boilerplate at the beginning and end of the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f98c49-e16a-4f0f-924d-b9cf644b21f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99959a79-92d8-4fcc-9b40-12839e0a0c9f",
   "metadata": {},
   "source": [
    "We can again use [regular expressions](https://www.oreilly.com/content/an-introduction-to-regular-expressions/) to match whitespace or more elaborate patterns. In this case, hyphenated words are split into separate tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef91cbbe-ff58-46ea-a493-50aa3da821ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "words = re.split(r'\\W+', text)\n",
    "\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e978ca5c-0d96-4771-b0dc-73a5a577a374",
   "metadata": {},
   "source": [
    "Now let's say we want to remove punctuation. We can get a convenient set of punctuation characters from Python's standard library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406c6a1d-a6ab-4ce0-a189-c74ac8aa6c31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re \n",
    "import string \n",
    "\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c1a8cb-17fe-47d5-81b0-e56d86038606",
   "metadata": {},
   "source": [
    "We can then construct a character set using a regular expression by using these punctuation characters, and remove said punctuation characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b814f0e-b4c4-43a2-8dc5-475e1f4a6310",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_punct = re.compile(f'[{re.escape(string.punctuation)}]')\n",
    "stripped = [regex_punct.sub('', w) for w in words]\n",
    "stripped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d331fe-20ab-4045-a647-62083f20df3d",
   "metadata": {},
   "source": [
    "We probabably should concern ourselves with making the casing consistent, as in uppercase or lowercase and making sure one convention is stuck to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6273f64-0244-4fa5-a5b6-eab41a5a71fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowercased = [w.lower() for w in stripped]\n",
    "lowercased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20206d23-4984-4442-a30a-7491a9585404",
   "metadata": {},
   "source": [
    "This was a a simple example, using simple clean text with some simple cleaning operations. This is obviously an ideal format to work with text data but it is not always this clean. Sometimes you may have PDF's that have text as images, or social media posts filled with typos and user grammar errors. You may even find domain-specific vocabularly you will not find in a dictionary, or documents with lots of numeric data that really should not be treated as words. You should always strive for simplicity first, and escalate the complexity of the data and its cleaning accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011bca5c-c761-457f-bdba-1c7a3bca78e7",
   "metadata": {},
   "source": [
    "## Using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b16c4ae-2cfc-41c7-804f-115d6bc00573",
   "metadata": {},
   "source": [
    "The Natural Language Toolkit (NLTK) is a Python library for processing and working with text. We can use it clean text and get it read for machine learning applications. \n",
    "\n",
    "You will need to install NLTK using pip. \n",
    "\n",
    "```\n",
    "pip install -U nltk\n",
    "```\n",
    "\n",
    "You will also need to download all the data for the library. \n",
    "\n",
    "```\n",
    "python -m nltk.downloader all\r",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d07b89-1e58-4b6b-8009-6f1d8b360ccf",
   "metadata": {},
   "source": [
    "### Breaking Up Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8417bf2-3076-4c39-8573-ad7dafb03915",
   "metadata": {},
   "source": [
    "We can split up words in NLTK using the `word_tokenize()` function. It will split on white space and punctuation including commas, periods, and contractions like `what's -> what 's`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e10c53-772d-4247-a07d-ecd10a20a3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(text)\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd99845d-98e6-42e8-9816-bacc0d7378d5",
   "metadata": {},
   "source": [
    "You will see here that the tokens above have punctionation marks as separate tokens. We can filter those out if we like using the `is_alpha()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5f3a73-4e75-4a04-abe1-5805bc065a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_puncts = [w for w in words if w.isalpha()]\n",
    "no_puncts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d0ba21-2052-49aa-88ca-1dcf24994a60",
   "metadata": {},
   "source": [
    "### Breaking Up Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438f7239-b1a1-4cf5-88fa-0b299b1bb834",
   "metadata": {},
   "source": [
    "Another way we can process this text is to break it up into sentences rather than words. We can bring in the `sent_tokenize()` function from NLTK to achieve this. We can then grab the 25th sentence in the story. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2284b60-26ff-4a08-9af7-fed256d5589d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences[25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd77bbc-8550-46c8-87a5-d1e4145e6073",
   "metadata": {},
   "source": [
    "### Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a665085-69fb-4d9c-9097-11ffb1eb9b33",
   "metadata": {},
   "source": [
    "Another task you might consider doing is removing **stop words**, which are words that bear little meaning like *the* and *is*. You look at stopwords available for English in NLTK. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aef3571-0cb1-445b-83d7-c2adf6191033",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2b24fc-e781-4def-8cf1-a1cabeafda9b",
   "metadata": {},
   "source": [
    "We can take these stop words, package them into a set, and remove them from our text. Note because the stop words are in lower case, we should compare each word in lower case as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c829145-aeea-4806-a905-f3e4a4208b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stop_words = [w for w in no_puncts if not w.lower() in stop_words]\n",
    "no_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3921e5a2-579e-4991-bd9b-a27d117c0124",
   "metadata": {},
   "source": [
    "### Stemming "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f236141-1181-429d-a508-e1ecc0620426",
   "metadata": {},
   "source": [
    "There might be times you want to reduce each word to its root or base. The words *fighter* and *fighting* stem from *fight*. This can help reduce the vocabularly and find broader tones or sentiments in the document. The most popular stemming algorithm is the Porter Stemming algorithm which NLTK has available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033602a9-c535-4a26-863b-d7d31f0bcea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "stemmed = [porter.stem(word) for word in no_stop_words]\n",
    "stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40295323-d970-4800-94cd-db71be6189d2",
   "metadata": {},
   "source": [
    "There are also **lemmatization** tools in NLTK, which help group and consolidate terms. For example, \"better\" has the word \"good\" as its lemma, and \"was\" has \"be.\" We will talk more about lemmatization with spaCy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de54b1f-2997-4f73-838a-6ecc149bf099",
   "metadata": {},
   "source": [
    "## Using spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581e48bb-4aaa-425d-878c-518dfc8fb88b",
   "metadata": {},
   "source": [
    "While NLTK is a great library, another that has grown popular for its scalability and efficiency is [spaCy](https://spacy.io/). We'll cover a few of its features here.  \n",
    "\n",
    "First install spaCy as well as its English model. \n",
    "\n",
    "```\n",
    "pip install spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "After that, you should be set to run spacy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a2bc14-70dc-4585-ba08-ac8fd69a87aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221dbd54-7705-4bd3-af06-ca2ebd4594c3",
   "metadata": {},
   "source": [
    "Let's load up Sleepy Hollow but this time into a spaCy doc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fd8d92-d688-498a-9fd0-6d508325d66b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sleepy_hollow = nlp(text)\n",
    "type(sleepy_hollow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944fdf27-3a7b-4730-8f8c-874e214a63ec",
   "metadata": {},
   "source": [
    "We can traverse the text tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6f8ae7-c838-4737-b9a0-160888e531ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "[token.text for token in sleepy_hollow]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd359e1-1fb8-40fa-ac87-551350d21a9b",
   "metadata": {},
   "source": [
    "We can also traverse the sentences, which are packaged into `Span` objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9361b87-7e71-474e-93c0-f8ad3d981bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "[token.text for token in sleepy_hollow.sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d2ffb9-e16f-4217-a7f7-a3cb97070dea",
   "metadata": {},
   "source": [
    "There are a lot of helpful attributes with each token in spaCy. Below we iterate a handful of tokens from the Sleepy Hollow document and print a few attributes we learned about previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e4329f-f234-4a91-9c56-00a806baf8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in sleepy_hollow[50:60]: \n",
    "    print(f\"Index: {token.idx}\")\n",
    "    print(f\"Text: {token.text}\")\n",
    "    print(f\"Is Alpha: {token.is_alpha}\")\n",
    "    print(f\"Is Punctuation: {token.is_punct}\")\n",
    "    print(f\"Is Stop Word: {token.is_stop}\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d333320-399b-44cc-b81a-9a7165d3cfa6",
   "metadata": {},
   "source": [
    "You can also implement your own tokenization procedures but we will keep the scope focused for now. Let's take a look at the lemmatization of each token. Sure enough, spaCy will find the lemma of each word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99a4325-7534-42f9-9517-22da7c817e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in sleepy_hollow: \n",
    "    if token.is_alpha:\n",
    "        print(f\"{token.text} -> {token.lemma_}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306252b7-3662-4705-88de-b7a885ba0488",
   "metadata": {},
   "source": [
    "This should give us enough tools and exposure to text cleaning. Just be wary that how you clean your text data is really driven by what you want to achieve and the state of the data itself. We had a nice clean short story to work with here, with an ideal UTF8 text body with no markup from HTML or PDF. There will be times you have to handle domain specific words and language, and decide to remove mathematical symbols like numbers and dates which may not be useful for your language model. Then there are simple but tedious matters like typos and errors, all of which might need to be handled for your large language model. \n",
    "\n",
    "Consider saving and documenting your cleaning steps too! Make reusable pipelines for your projects and perhaps even save the cleaned documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e969902-dd38-4ea8-9fe8-8d7ba5061c18",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Take this excerpt from an Edgar Allen Poe poem and tokenize it with the tool of your choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0983b092-a318-4eb0-939f-3fc64e252ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Once upon a midnight dreary, While I pondered, weak and weary\"\n",
    "\n",
    "# build your model below "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c99a825-0e76-4630-ad77-ef02781a42db",
   "metadata": {},
   "source": [
    "### SCROLL DOWN FOR ANSWER\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "v "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b398a720-ee26-49ed-93b0-0e6e5e307cb9",
   "metadata": {},
   "source": [
    "## Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a2f392-f10e-46b7-966c-573b7c07c622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "poem = word_tokenize(text)\n",
    "\n",
    "for w in poem: \n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c730a80c-f5c3-4477-afc2-0834e1c8abf5",
   "metadata": {},
   "source": [
    "### Using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d91a20-d8f5-453b-905c-f55779a5e84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "poem = nlp(text)\n",
    "\n",
    "for w in poem: \n",
    "    print(w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
