{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f66022a8-d6b3-478e-a11a-e1fd5164fdd7",
   "metadata": {},
   "source": [
    "# Vectorizing and Encoding Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e6744e-6f63-4c95-93a7-2a01c789a9bb",
   "metadata": {},
   "source": [
    "In this section we will learn how to prepare text data into numeric data that machine learning models can understand. This process is called **vectorization**, where we create numeric representations of words and tokens. There are many ways to achieve this task, and we will get our hands on a few techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350e8832-93e2-4c93-b773-c91680d42910",
   "metadata": {},
   "source": [
    "## Turning Text into Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1ed516-1e47-4a8e-9222-b9f09ed8e673",
   "metadata": {},
   "source": [
    "Recall this excerpt from the Charles Dickens novel in UTF-8 (Unicode Transformation Format â€“ 8-bit) format. Because it is 8-bit, that means the computer stores/reads each character as 8 bits of memory. Recall a binary number is a numeric system limited to only two digits 0 and 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22b1972-a100-411a-870a-f696101dea00",
   "metadata": {},
   "source": [
    "> It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way--in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only.\n",
    "\n",
    "```\n",
    "01001001 01110100 00100000 01110111 01100001 01110011 00100000 01110100 01101000 01100101 00100000 01100010 01100101 01110011 01110100 00100000 01101111 01100110 00100000 01110100 01101001 01101101 01100101 01110011 00101100 00100000 01101001 01110100 00100000 01110111 01100001 01110011 00100000 01110100 01101000 01100101 00100000 01110111 01101111 01110010 01110011 01110100 00100000 01101111 01100110 00100000 01110100 01101001 01101101 01100101 01110011 00101100 00100000 01101001 01110100 00100000 01110111 01100001 01110011 00100000 01110100 01101000 01100101 00100000 01100001 01100111 01100101 00100000 01101111 01100110 00100000 01110111 01101001 01110011 01100100 01101111 01101101 00101100 00100000 01101001 01110100 00100000 01110111 01100001 01110011 00100000 01110100 01101000 01100101 00100000 01100001 01100111 01100101 00100000 01101111 01100110 00100000 01100110 01101111 01101111 01101100 01101001 01110011 01101000 01101110 01100101 01110011 01110011 00101100 00100000 01101001 01110100 00100000 01110111 01100001 01110011 00100000 01110100 01101000 01100101 00100000 01100101 01110000 01101111 01100011 01101000 00100000 01101111 01100110 00100000 01100010 01100101 01101100 01101001 01100101 01100110 00101100 00100000 01101001 01110100 00100000 01110111 01100001 01110011 00100000 01110100 01101000 01100101 00100000 01100101 01110000 01101111 01100011 01101000 00100000 01101111 01100110 00100000 01101001 01101110 01100011 01110010 01100101 01100100 01110101 01101100 01101001 01110100 01111001 00101100 00100000 01101001 01110100 00100000 01110111 01100001 01110011 00100000 01110100 01101000 01100101 00100000 01110011 01100101 01100001 01110011 01101111 01101110 00100000 01101111 01100110 00100000 01001100 01101001 01100111 01101000 01110100 00101100 00100000 01101001 01110100 00100000 01110111 01100001 01110011 00100000 01110100 01101000 01100101 00100000 01110011 01100101 01100001 01110011 01101111 01101110 00100000 01101111 01100110 00100000 01000100 01100001 01110010 01101011 01101110 01100101 01110011 01110011 00101100 00100000 01101001 01110100 00100000 01110111 01100001 01110011 00100000 01110100 01101000 01100101 00100000 01110011 01110000 01110010 01101001 01101110 01100111 00100000 01101111 01100110 00100000 01101000 01101111 01110000 01100101 00101100 00100000 01101001 01110100 00100000 01110111 01100001 01110011 00100000 01110100 01101000 01100101 00100000 01110111 01101001 01101110 01110100 01100101 01110010 00100000 01101111 01100110 00100000 01100100 01100101 01110011 01110000 01100001 01101001 01110010 00101100 00100000 01110111 01100101 00100000 01101000 01100001 01100100 00100000 01100101 01110110 01100101 01110010 01111001 01110100 01101000 01101001 01101110 01100111 00100000 01100010 01100101 01100110 01101111 01110010 01100101 00100000 01110101 01110011 00101100 00100000 01110111 01100101 00100000 01101000 01100001 01100100 00100000 01101110 01101111 01110100 01101000 01101001 01101110 01100111 00100000 01100010 01100101 01100110 01101111 01110010 01100101 00100000 01110101 01110011 00101100 00100000 01110111 01100101 00100000 01110111 01100101 01110010 01100101 00100000 01100001 01101100 01101100 00100000 01100111 01101111 01101001 01101110 01100111 00100000 01100100 01101001 01110010 01100101 01100011 01110100 00100000 01110100 01101111 00100000 01001000 01100101 01100001 01110110 01100101 01101110 00101100 00100000 01110111 01100101 00100000 01110111 01100101 01110010 01100101 00100000 01100001 01101100 01101100 00100000 01100111 01101111 01101001 01101110 01100111 00100000 01100100 01101001 01110010 01100101 01100011 01110100 00100000 01110100 01101000 01100101 00100000 01101111 01110100 01101000 01100101 01110010 00100000 01110111 01100001 01111001 00101101 00101101 01101001 01101110 00100000 01110011 01101000 01101111 01110010 01110100 00101100 00100000 01110100 01101000 01100101 00100000 01110000 01100101 01110010 01101001 01101111 01100100 00100000 01110111 01100001 01110011 00100000 01110011 01101111 00100000 01100110 01100001 01110010 00100000 01101100 01101001 01101011 01100101 00100000 01110100 01101000 01100101 00100000 01110000 01110010 01100101 01110011 01100101 01101110 01110100 00100000 01110000 01100101 01110010 01101001 01101111 01100100 00101100 00100000 01110100 01101000 01100001 01110100 00100000 01110011 01101111 01101101 01100101 00100000 01101111 01100110 00100000 01101001 01110100 01110011 00100000 01101110 01101111 01101001 01110011 01101001 01100101 01110011 01110100 00100000 01100001 01110101 01110100 01101000 01101111 01110010 01101001 01110100 01101001 01100101 01110011 00100000 01101001 01101110 01110011 01101001 01110011 01110100 01100101 01100100 00100000 01101111 01101110 00100000 01101001 01110100 01110011 00100000 01100010 01100101 01101001 01101110 01100111 00100000 01110010 01100101 01100011 01100101 01101001 01110110 01100101 01100100 00101100 00100000 01100110 01101111 01110010 00100000 01100111 01101111 01101111 01100100 00100000 01101111 01110010 00100000 01100110 01101111 01110010 00100000 01100101 01110110 01101001 01101100 00101100 00100000 01101001 01101110 00100000 01110100 01101000 01100101 00100000 01110011 01110101 01110000 01100101 01110010 01101100 01100001 01110100 01101001 01110110 01100101 00100000 01100100 01100101 01100111 01110010 01100101 01100101 00100000 01101111 01100110 00100000 01100011 01101111 01101101 01110000 01100001 01110010 01101001 01110011 01101111 01101110 00100000 01101111 01101110 01101100 01111001 00101110\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccfb17c-ca74-4897-9b34-47a275b50a1f",
   "metadata": {},
   "source": [
    "While this is how the computer really sees the text behind-the-scenes, there are further mathematical transformations that have to occur. Thankfully you will not have to think in 1's and 0's as Python will abstract that away from you. However, it is still good to be aware of what 8-bit versus 16-bit means, and UTF-8 stores a character as 8 bits of data but technically extends to 32 bits as it can also store in 4 byte chunks. Now of course, this limits the number of characters UTF-8 can support and map to, but it's a pretty common format and supports internationalization robustly as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f4e258-fb1e-4576-ab1d-a37bb049bf5a",
   "metadata": {},
   "source": [
    "When we do natural language processing and large language models, we have to convert documents into fixed-length vectors of numbers. A common approach is to use the **Bag-of-Words (BoW)** model which will be covered a lot in the remainder of this course. It typically disregards the order the words occur and focuses on the occurrence of words in a document. Each word gets a unique number assigned and a vector matching the length of the known vocabulary flags whether that word has occurred or not in its indexed position. \n",
    "\n",
    "There are a couple of ways to implement a Bag-of-Words model. Let's start with the `CountVectorizer`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6946fe10-3737-4b94-91c1-1dd36a7aa634",
   "metadata": {},
   "source": [
    "## Word Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c79ea7-4cfe-4702-a15a-4371ad86afc3",
   "metadata": {},
   "source": [
    "The `CountVectorizer` in scikit-learn will tokenize text in documents and build a vocabulary of known words. Then with new documents you can encode vectors that align to that vocabulary. \n",
    "\n",
    "Here is a simple example of using the `CountVectorizer`. Study it and the output closely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7d3ad2-3379-4a43-93cd-eecd94b8652e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = [\"The sun is up. The sun is yellow. The yellow sun is over the house.\"]\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# tokenize \n",
    "vectorizer.fit(text)\n",
    "\n",
    "# show vocabulary\n",
    "print(vectorizer.vocabulary_)\n",
    "\n",
    "# encode a new document\n",
    "vector = vectorizer.transform([\"The house is yellow. The sun is over the house.\"])\n",
    "\n",
    "# summarize vector\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8efa9e-922f-493b-87de-edac1b58a819",
   "metadata": {},
   "source": [
    "Notice closely that we start with the text \"The sun is up. The sun is yellow. The yellow sun is over the house.\" This is `fit()` to the `CountVectorizer` and that is our entire universe of vocabularly in that sentence.  Just those 7 words _the, sun, is, up, yellow, over, and house_. I print that `vocabulary_` and it conveniently returns a map showing which word occupies which index in a vector. \n",
    "\n",
    "When I `transform()` a new document \"The house is yellow. The sun is over the house.\" it turns it into a vector with those counts for each word. Again notice that each vocabularly word holds a given position index on the array. \n",
    "\n",
    "What happens if we give it a document with vocabularly it has not seen before? This sentence below contains the word \"blue\" and \"moon\" which we did not `fit()` to previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00ff58a-7575-493e-a22a-417309810bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = vectorizer.transform([\"The house is blue. The moon is over the house.\"])\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeb3483-3a2c-489d-afa1-bd4c8e449620",
   "metadata": {},
   "source": [
    "Basically any unknown words like \"blue\" and \"moon\" are ignored. It is not going to extend the array for new vocabularly. To do that, I would have to call the `fit()` function again which will rebuild the vocabularly. Note that previously captured words will not necessarily retain their old positions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13fd3d5-1063-48bf-a0a4-188727d0ff42",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"The sun is up. The sun is yellow. The yellow sun is over the house.\", \"The house is blue. The moon is over the house.\"]\n",
    "vectorizer.fit(text)\n",
    "print(vectorizer.vocabulary_)\n",
    "\n",
    "vector = vectorizer.transform([\"The house is blue. The moon is over the house.\"])\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8f3cc7-b21f-4a28-89e5-ff50fb29bc3e",
   "metadata": {},
   "source": [
    "We can also observe the vocabularly was converted to lowercase by default and punctuation was ignored. [scikit-learn provides a lot of options to change these behaviors.](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c807518-33a8-43b5-8a25-fcf9043c502e",
   "metadata": {},
   "source": [
    "## Word Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e76c20-3cf1-42bc-9ff7-752b70312d46",
   "metadata": {},
   "source": [
    "Counting words can only get us so far. Unhelpful words like _the_ will appear many times and will bloat counts in vectors. A far more popular method that works around this is **Term Frequency - Inverse Document Frequency (TF-IDF)** technique. It scores words that are more _interesting_ and diminishes words that appear a lot across documents. \n",
    "\n",
    "It will tokenize documents, build the vocabularly, apply some inverse document frequency weightings, and then be able to encode new documents. It actually performs the same steps as the `CountVectorizer` but adds some new ones for the inversion logic. Note that separating text into separate documents will have a different result than putting all text in a single document, as this math formula works on the document-level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48e2b90-e782-4694-915f-4bbe6fde88c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text = [\"The sun is up.\", \"The sun is yellow.\", \"The yellow sun is over the house.\"]\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# tokenize \n",
    "vectorizer.fit(text)\n",
    "\n",
    "# show vocabulary and scores \n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)\n",
    "\n",
    "# encode a new document\n",
    "vector = vectorizer.transform([\"The house is yellow.\"])\n",
    "\n",
    "# summarize vector\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66bf481-9a51-496d-bd44-056e52d90cff",
   "metadata": {},
   "source": [
    "Above the TF-IDF vectorizer learned 7 words. The words \"the\", \"is\", and \"sun\" have the lowest score of 1 while \"house\", \"over\", and \"up\" are the highest-scoring words. When we score a new document, we get normalized scores scaled between 0 and 1 for each word. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fb78a0-06c6-42f2-be7d-7c6c44a97d78",
   "metadata": {},
   "source": [
    "## Word Hashing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c186876-2850-4b04-8816-de87f473e625",
   "metadata": {},
   "source": [
    "With the previous two techniques, we can get enormous vocabularies and it might be nice to consider downsizing them in some algorithmic way. This is where **hashing**, or a one-way conversion of words to integers, can be handy. No vocabularly is required and you can choose any vector length you want (the default is $ 2^{20} $). Given a lot of AI techniques are a one-way conversion anyway, hashing might fit the bill here. \n",
    "\n",
    "We use the [`HashingVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html) class to hash words, tokenize, and encode documents. Choosing the vector length is the largest decision in this technique. The default value for `n_features` of $ 2^{20} $ should be sufficient in most cases. The vector length has to be large enough to prevent clashing, but having it larger will take more memory. We will use a very small vector of `20` just to make our toy example easy to observe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaffb1c-50a6-4717-a3c6-2b93c7a55935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "text = [\"The sun is up.\", \"The sun is yellow.\", \"The yellow sun is over the house.\"]\n",
    "vectorizer = HashingVectorizer(n_features=20) # default n_features = 2**20\n",
    "\n",
    "# tokenize \n",
    "vectorizer.fit(text)\n",
    "\n",
    "# encode a new document\n",
    "vector = vectorizer.transform([\"The house is yellow.\"])\n",
    "\n",
    "# summarize vector\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf3d106-3f77-48fb-b94a-69d15eda28d0",
   "metadata": {},
   "source": [
    "Notice if I use a larger vector, it is going to be much harder to observe the result due to the vector length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b3829b-efd9-44a0-a924-b74023df6a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "text = [\"The sun is up.\", \"The sun is yellow.\", \"The yellow sun is over the house.\"]\n",
    "vectorizer = HashingVectorizer(n_features=2**10) # default n_features = 2**20\n",
    "\n",
    "# tokenize \n",
    "vectorizer.fit(text)\n",
    "\n",
    "# encode a new document\n",
    "vector = vectorizer.transform([\"The house is yellow.\"])\n",
    "\n",
    "# summarize vector\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf65636-8485-4a82-9bf3-fbb08ce0cd96",
   "metadata": {},
   "source": [
    "## Binary and Other Parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e15247-8f38-4d7c-988e-ad9835e11fcb",
   "metadata": {},
   "source": [
    "I highly encourage exploring the documentation available for [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer),  [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html), and [HashingVecorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html). Some useful transformations can be added. For example, below we turn off the automatic conversion to lowercase and we use binary outcomes of words instead of counts, which can be useful for certain probability models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa2bfcf-a2c9-4a09-8209-683fac0b9d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text = [\"The sun is up. The sun is yellow. The yellow sun is over the house.\"]\n",
    "vectorizer = CountVectorizer(binary=True, lowercase=False)\n",
    "\n",
    "# tokenize \n",
    "vectorizer.fit(text)\n",
    "\n",
    "# show vocabulary\n",
    "print(vectorizer.vocabulary_)\n",
    "\n",
    "# encode a new document\n",
    "vector = vectorizer.transform([\"The house is yellow. The sun is over the house.\"])\n",
    "\n",
    "# summarize vector\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0f8016-d630-4720-a42d-c81bab934214",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e21d99b-e5cf-4ca6-a91e-fa1404fb3146",
   "metadata": {},
   "source": [
    "Create a simple vectorizer that flags in binary fashion whether each vocabulary word from *The Legend of Sleepy Hollow* occurred in the new document or not. Complete this code below by replacing the question marks \"?\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420a2e03-eca1-4f46-a353-eaba69805edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ?\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(threshold=np.inf) # don't truncate vector outputs \n",
    "\n",
    "# bring in document\n",
    "filename = 'legend_of_sleepy_hollow.txt' \n",
    "file = open(filename, encoding=\"utf-8\")\n",
    "text = file.read()\n",
    "file.close()\n",
    "text\n",
    "\n",
    "# construct vectorizer \n",
    "vectorizer = ?\n",
    "\n",
    "# tokenize \n",
    "vectorizer.fit(?)\n",
    "\n",
    "# encode a new document\n",
    "vector = vectorizer.transform([\"The headless horseman was a Hessian trooper.\"])\n",
    "\n",
    "# summarize vector\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbce05c-4103-4da1-9e91-08d3ea107b20",
   "metadata": {},
   "source": [
    "### SCROLL DOWN FOR ANSWER\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "v "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca16fe9-8278-4a7a-9dcb-a8c9b5c491b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(threshold=np.inf) # don't truncate vector outputs \n",
    "\n",
    "# bring in document\n",
    "filename = 'legend_of_sleepy_hollow.txt' \n",
    "file = open(filename, encoding=\"utf-8\")\n",
    "text = file.read()\n",
    "file.close()\n",
    "text\n",
    "\n",
    "# construct vectorizer \n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "\n",
    "# tokenize \n",
    "vectorizer.fit([text])\n",
    "\n",
    "# encode a new document\n",
    "vector = vectorizer.transform([\"The headless horseman was a Hessian trooper.\"])\n",
    "\n",
    "# summarize vector\n",
    "print(vector.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
